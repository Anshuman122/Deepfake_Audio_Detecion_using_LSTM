# Deepfake_Audio_Detecion_using_LSTM
Audio Deepfake Detection using LSTM is a deep learning-based project designed to identify AI-generated (fake) audio by analyzing temporal and spectral features of speech. Leveraging the ASVspoof dataset, the system extracts Mel-Frequency Cepstral Coefficients (MFCCs) from both real and fake audio clips using Librosa, and feeds them into a Long Short-Term Memory (LSTM) model built with TensorFlow/Keras. The model is trained to detect subtle inconsistencies in speech patterns that are typical of synthetic audio. The project achieves high performance in distinguishing real from fake audio, evaluated using metrics such as accuracy, precision, recall, and F1-score. It also includes preprocessing, visualization, and evaluation scripts, and is implemented using Python libraries such as NumPy, Pandas, Matplotlib, and Scikit-learn. This project showcases practical skills in audio feature extraction, time-series modeling, and AI-based threat detection, with future scope for real-time deployment and integration into web or mobile applications. 
