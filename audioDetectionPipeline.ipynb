{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install librosa\n",
    "!pip install tensorflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU not fount install toolkit\")\n",
    "else:\n",
    "    print(\"GPU available.\")\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Dropout, Masking, Input, \n",
    "                                     Bidirectional, Conv1D, MaxPooling1D, BatchNormalization)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MFCC = 13\n",
    "MAX_N_FFT = 2048\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(audio_path, label, n_mfcc=N_MFCC, max_n_fft=MAX_N_FFT):\n",
    "    try:\n",
    "        audio_signal, sample_rate = librosa.load(audio_path, sr=None)\n",
    "        n_fft = min(max_n_fft, len(audio_signal))\n",
    "        mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft)\n",
    "        return mfccs.T, label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class AudioDataGenerator(Sequence):\n",
    "    def __init__(self, real_paths, fake_paths, batch_size, n_mfcc=N_MFCC, max_n_fft=MAX_N_FFT):\n",
    "        self.real_paths = real_paths\n",
    "        self.fake_paths = fake_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.max_n_fft = max_n_fft\n",
    "\n",
    "        \n",
    "        min_samples = min(len(real_paths), len(fake_paths))\n",
    "        self.real_paths = random.sample(real_paths, min_samples)\n",
    "        self.fake_paths = random.sample(fake_paths, min_samples)\n",
    "\n",
    "       \n",
    "        self.data_paths = list(zip(self.real_paths, [0] * len(self.real_paths))) + \\\n",
    "                          list(zip(self.fake_paths, [1] * len(self.fake_paths)))\n",
    "        random.shuffle(self.data_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.data_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X, y = [], []\n",
    "\n",
    "        for audio_path, label in batch_data:\n",
    "            features, label = load_and_preprocess_data(audio_path, label, \n",
    "                                                       self.n_mfcc, self.max_n_fft)\n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(label)\n",
    "\n",
    "        X_padded = pad_sequences(X, padding='post', dtype='float32')\n",
    "        return np.array(X_padded), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_audio_dir = r\"C:\\\\Users\\\\akn\\Desktop\\\\audio_cllg_hackathons\\\\audio\\\\audio\\\\real\"\n",
    "fake_audio_dir = r\"C:\\\\Users\\\\akn\\\\Desktop\\\\audio_cllg_hackathons\\\\audio\\\\audio\\\\fake\"\n",
    "\n",
    "\n",
    "\n",
    "real_audio_paths = [os.path.join(real_audio_dir, file) for file in os.listdir(real_audio_dir) if file.endswith('.flac')]\n",
    "fake_audio_paths = [os.path.join(fake_audio_dir, file) for file in os.listdir(fake_audio_dir) if file.endswith('.flac')]\n",
    "\n",
    "\n",
    "X_train_real, X_val_real = train_test_split(real_audio_paths, test_size=0.2, random_state=42)\n",
    "X_train_fake, X_val_fake = train_test_split(fake_audio_paths, test_size=0.2, random_state=42)\n",
    "X_val_real, X_test_real = train_test_split(X_val_real, test_size=0.5, random_state=42)\n",
    "X_val_fake, X_test_fake = train_test_split(X_val_fake, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = AudioDataGenerator(X_train_real, X_train_fake, batch_size=BATCH_SIZE)\n",
    "val_generator = AudioDataGenerator(X_val_real, X_val_fake, batch_size=BATCH_SIZE)\n",
    "test_generator = AudioDataGenerator(X_test_real, X_test_fake, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "       LSTM(64, return_sequences=True, input_shape=(None, N_MFCC)),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=7,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('real_batch_lstm_model_akn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_generator).argmax(axis=1)\n",
    "\n",
    "y_true = np.concatenate([batch[1] for batch in test_generator], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import librosa\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_and_preprocess_data(audio_path, label=None, n_mfcc=13, max_n_fft=2048):\n",
    "    try:\n",
    "        audio_signal, sample_rate = librosa.load(audio_path, sr=None)\n",
    "        n_fft = min(max_n_fft, len(audio_signal))\n",
    "        mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft)\n",
    "        return mfccs.T, label\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_unseen_audio(audio_path, n_mfcc=13, max_n_fft=2048):\n",
    "    features, label = load_and_preprocess_data(audio_path, label=None, n_mfcc=n_mfcc, max_n_fft=max_n_fft)\n",
    "    if features is not None:\n",
    "        return features\n",
    "    else:\n",
    "        print(f\"Error processing {audio_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(r\"real_batch_lstm_model_akn.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio(audio_path):\n",
    "    processed_audio = preprocess_unseen_audio(audio_path)\n",
    "\n",
    "    if processed_audio is not None:\n",
    "        processed_audio = np.expand_dims(processed_audio, axis=0)\n",
    "        processed_audio = pad_sequences(processed_audio, padding='post', dtype='float32')\n",
    "        predictions = model.predict(processed_audio)\n",
    "        predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "        if predicted_class == 0:\n",
    "            print(\"The audio is predicted to be real.\")\n",
    "        else:\n",
    "            print(\"The audio is predicted to be fake.\")\n",
    "\n",
    "        print(\"Prediction probabilities:\", predictions)\n",
    "    else:\n",
    "        print(\"Error: Failed to preprocess audio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124makn\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAudio\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mreal\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPA_E_2541263.flac\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpredict_audio\u001b[49m(audio_file_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_audio' is not defined"
     ]
    }
   ],
   "source": [
    "audio_file_path = r\"C:\\Users\\akn\\Desktop\\Audio\\real\\PA_E_2541263.flac\"\n",
    "predict_audio(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('batch_lstm_model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
